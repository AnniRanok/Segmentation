# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

"""

!pip install segmentation-models-pytorch==0.2.0

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import torch
import cv2
import sys
import collections
import albumentations as albu
import torchvision
import segmentation_models_pytorch as smp
from pathlib import Path
import shutil
from sklearn.model_selection import KFold

from torch.utils.data import DataLoader
from glob import glob
from os import path
from PIL import Image
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
from sklearn.metrics import jaccard_score
from torch.optim.lr_scheduler import StepLR
from torch.optim import SGD

# %matplotlib inline

IMG_WIDTH = 256
IMG_HEIGHT = 256
NUM_CLASSES = 46

BATCH_SIZE = 16
N_WORKERS = 1
NUM_EPOCHS = 30

DEVICE = 'cuda'

from google.colab import drive
drive.mount('/content/drive')

# Category names
import json
json_file_path = Path("/content/drive/My Drive/label_descriptions1.json")

with open(json_file_path) as f:
    label_descriptions = json.load(f)
label_names = [x['name'] for x in label_descriptions['categories']]

!unzip /content/drive/MyDrive/fashion.zip "train/*" -d /content

root_path_train = './train'
df_path_train = '/content/drive/My Drive/train2.csv'

image_df = pd.read_csv(df_path_train)
image_df

FOLD = 0
N_FOLDS = 5
image_dir = "/content/train"  # images directory
root_path_train = "/content/train"  # root to save new folders
image_extension = ".jpg"  # Image enhancement
# Creating a KFold
kf = KFold(n_splits=N_FOLDS, random_state=42, shuffle=True)
splits = kf.split(image_df)

# Function for obtaining training and validation data
def get_fold():
    for i, (train_index, valid_index) in enumerate(splits):
        if i == FOLD:
            return image_df.iloc[train_index], image_df.iloc[valid_index]

# We get a training and validation dataframe
train_df, valid_df = get_fold()

# We create new directories for training and validation images
train_dir = os.path.join(root_path_train, 'train_images')
valid_dir = os.path.join(root_path_train, 'valid_images')

os.makedirs(train_dir, exist_ok=True)
os.makedirs(valid_dir, exist_ok=True)

# Function for moving images
def move_images(df, source_dir, target_dir, extension=".jpg"):
    for index, row in df.iterrows():
        img_filename = row['ImageId'] + extension  # Add the extension to the file name
        img_path = os.path.join(source_dir, img_filename)  # The path to the original image
        if os.path.exists(img_path):
            shutil.copy(img_path, target_dir)  # Move the image to a new directory

# Moving images to training and validation folders
move_images(train_df, image_dir, train_dir, extension=image_extension)
move_images(valid_df, image_dir, valid_dir, extension=image_extension)

# Saving CSV files for training and validation
train_df.to_csv("train_data.csv", index=False)
valid_df.to_csv("valid_data.csv", index=False)

# Load the training and validation data from CSV files
train_df = pd.read_csv("train_data.csv")
val_df = pd.read_csv("valid_data.csv")
image_dir='./train'

# With augmentation
class FashionDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, df, height, width, transforms=None):
        self.transforms = transforms
        self.image_dir = image_dir
        self.df = df
        self.height = height
        self.width = width
        self.image_info = collections.defaultdict(dict)

        # Column processing and aggregation
        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split("_")[0])
        temp_df = self.df.groupby('ImageId')[['EncodedPixels', 'CategoryId']].agg(lambda x: list(x)).reset_index()
        size_df = self.df.groupby('ImageId')[['Height', 'Width']].mean().reset_index()
        temp_df = temp_df.merge(size_df, on='ImageId', how='left')

        # Saving image information
        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):
            image_id = row['ImageId']
            image_path = os.path.join(self.image_dir, f"{image_id}.jpg")
            if not os.path.isfile(image_path):
                print(f"Warning: {image_path} does not exist.")
                continue

            # We store metadata about images
            self.image_info[index] = {
                "image_id": image_id,
                "image_path": image_path,
                "width": self.width,
                "height": self.height,
                "labels": row["CategoryId"],
                "orig_height": row["Height"],
                "orig_width": row["Width"],
                "annotations": row["EncodedPixels"]
            }

        self.img2tensor = torchvision.transforms.ToTensor()

    def rle_decode(self, rle, shape):
        """Decodes a mask encoded in RLE format."""
        img = np.zeros(shape[0] * shape[1], dtype=np.uint8)
        s = list(map(int, rle.split()))
        starts = s[0::2]
        lengths = s[1::2]
        for start, length in zip(starts, lengths):
            img[start:start + length] = 1
        return img.reshape(shape).T

    def __getitem__(self, idx):
    info = self.image_info[idx]
    img_path = info["image_path"]
    img = Image.open(img_path).convert("RGB")
    img = img.resize((self.width, self.height), resample=Image.BILINEAR)

    mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)
    labels = []

    for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):
        try:
            orig_height = int(float(info['orig_height']))
            orig_width = int(float(info['orig_width']))
        except ValueError:
            orig_height, orig_width = self.height, self.width

        sub_mask = self.rle_decode(annotation, (orig_height, orig_width))
        sub_mask = Image.fromarray(sub_mask)
        sub_mask = sub_mask.resize((self.width, self.height), resample=Image.NEAREST)
        mask[m] = np.array(sub_mask)

        labels.append(int(label) + 1)

    boxes = []
    new_labels = []
    new_masks = []

    for i in range(len(labels)):
        pos = np.where(mask[i, :, :])
        if pos[0].size == 0:  
            continue
        xmin, ymin = np.min(pos[1]), np.min(pos[0])
        xmax, ymax = np.max(pos[1]), np.max(pos[0])
        if abs(xmax - xmin) >= 20 and abs(ymax - ymin) >= 20:  
            boxes.append([xmin, ymin, xmax, ymax])
            new_labels.append(labels[i])
            new_masks.append(mask[i])

    if len(new_labels) == 0:
        boxes.append([0, 0, 20, 20])
        new_labels.append(0)
        new_masks.append(mask[0])

    nmx = np.stack(new_masks, axis=0)
    target = {
        "boxes": torch.as_tensor(boxes, dtype=torch.float32),
        "labels": torch.as_tensor(new_labels, dtype=torch.int64),
        "masks": torch.as_tensor(nmx, dtype=torch.uint8)
    }

    img = self.img2tensor(img)

    if self.transforms:
        augmented = self.transforms(image=np.array(img), masks=list(new_masks))
        img = Image.fromarray(augmented['image'])
        mask = np.stack(augmented['masks'], axis=0)
        target["masks"] = torch.as_tensor(mask, dtype=torch.uint8)

    return img, target



    def __len__(self):
        return len(self.image_info)

# Without augmentation
class FashionDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, df, height, width, transforms=None):
        self.transforms = transforms
        self.image_dir = image_dir
        self.df = df
        self.height = height
        self.width = width
        self.image_info = collections.defaultdict(dict)

        # Column processing and aggregation
        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split("_")[0])
        temp_df = self.df.groupby('ImageId')[['EncodedPixels', 'CategoryId']].agg(lambda x: list(x)).reset_index()
        size_df = self.df.groupby('ImageId')[['Height', 'Width']].mean().reset_index()
        temp_df = temp_df.merge(size_df, on='ImageId', how='left')

        # Saving image information
        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):
            image_id = row['ImageId']
            image_path = os.path.join(self.image_dir, f"{image_id}.jpg")
            if not os.path.isfile(image_path):
                print(f"Warning: {image_path} does not exist.")
                continue

            # We store metadata about images
            self.image_info[index] = {
                "image_id": image_id,
                "image_path": image_path,
                "width": self.width,
                "height": self.height,
                "labels": row["CategoryId"],
                "orig_height": row["Height"],
                "orig_width": row["Width"],
                "annotations": row["EncodedPixels"]
            }

        self.img2tensor = torchvision.transforms.ToTensor()

    def rle_decode(self, rle, shape):
        """Decodes a mask encoded in RLE format."""
        img = np.zeros(shape[0] * shape[1], dtype=np.uint8)
        s = list(map(int, rle.split()))
        starts = s[0::2]
        lengths = s[1::2]
        for start, length in zip(starts, lengths):
            img[start:start + length] = 1
        return img.reshape(shape).T

    def __getitem__(self, idx):
        # Download image and annotations
        info = self.image_info[idx]
        img_path = info["image_path"]
        img = Image.open(img_path).convert("RGB")
        img = img.resize((self.width, self.height), resample=Image.BILINEAR)

        mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)
        labels = []

        # We decode masks and prepare labels
        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):
            if isinstance(label, str):  # If label is a string, we convert it into a list
                label = [label]

            try:
                orig_height = int(float(info['orig_height']))
                orig_width = int(float(info['orig_width']))
            except ValueError:
                orig_height, orig_width = self.height, self.width

            sub_mask = self.rle_decode(annotation, (orig_height, orig_width))
            sub_mask = Image.fromarray(sub_mask)
            sub_mask = sub_mask.resize((self.width, self.height), resample=Image.BILINEAR)

            mask[m] = np.array(sub_mask)

            # If label is a list, we add each element
            if isinstance(label, list):  # If label is a list
                for lbl in label:
                    labels.append(int(lbl) + 1)
            else:
                labels.append(int(label) + 1)

        boxes = []
        new_labels = []
        new_masks = []

        # We create bounding frames and new masks
        for i in range(len(labels)):
            pos = np.where(mask[i, :, :])
            if pos[0].size == 0:  # If the mask is empty, skip it
                continue
            xmin, ymin = np.min(pos[1]), np.min(pos[0])
            xmax, ymax = np.max(pos[1]), np.max(pos[0])
            if abs(xmax - xmin) >= 20 and abs(ymax - ymin) >= 20:  # Ignore frames that are too small
                boxes.append([xmin, ymin, xmax, ymax])
                new_labels.append(labels[i])
                new_masks.append(mask[i])

        if len(new_labels) == 0:
            boxes.append([0, 0, 20, 20])
            new_labels.append(0)
            new_masks.append(mask[0])

        nmx = np.stack(new_masks, axis=0)
        target = {
            "boxes": torch.as_tensor(boxes, dtype=torch.float32),
            "labels": torch.as_tensor(new_labels, dtype=torch.int64),
            "masks": torch.as_tensor(nmx, dtype=torch.uint8)
        }

        img = self.img2tensor(img)

        if self.transforms:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.image_info)

def custom_collate(batch):
    images = []
    labels = []
    for img, label in batch:
        images.append(img)
        labels.append(label)

    return images, labels

train_augmentations = albu.Compose([
    albu.HorizontalFlip(p=0.5),
    albu.RandomBrightnessContrast(p=0.2),
    albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    albu.GaussianBlur(p=0.1),
])

datset_train = FashionDataset(
    image_dir=train_dir,
    df=train_df,
    height=IMG_HEIGHT,
    width=IMG_WIDTH,
    #transforms=train_augmentations
)

datset_val = FashionDataset(
    image_dir=train_dir,
    df=val_df,
    height=IMG_HEIGHT,
    width=IMG_WIDTH,
    #transforms=train_augmentations
)


data_loader_train = torch.utils.data.DataLoader(
    datset_train, batch_size=16, shuffle=True, num_workers=1,
    collate_fn=custom_collate)

data_loader_val = torch.utils.data.DataLoader(
    datset_val, batch_size=16, shuffle=True, num_workers=1,
    collate_fn=custom_collate)

class MyEpoch(smp.utils.train.Epoch):
    def _to_device(self):
        self.model.to(self.device)

    def run(self, dataloader):

        self.on_epoch_start()

        logs = {}
        loss_meter = smp.utils.meter.AverageValueMeter()

        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:
            for x, y in iterator:
                x = list(map(lambda x_el: x_el.to(self.device), x))
                y = list(map(lambda y_el: {k:v.to(self.device) for k,v in y_el.items()}, y))
                loss = self.batch_update(x, y)

                # update loss logs
                loss_value = loss.cpu().detach().numpy()
                loss_meter.add(loss_value)
                loss_logs = {'loss': loss_meter.mean}
                logs.update(loss_logs)

                if self.verbose:
                    s = self._format_logs(logs)
                    iterator.set_postfix_str(s)

        return logs

class TrainEpoch(MyEpoch):

    def __init__(self, model, loss, metrics, optimizer, device='cuda', verbose=True):
        super().__init__(
            model=model,
            loss=loss,
            metrics=metrics,
            stage_name='train',
            device=device,
            verbose=verbose,
        )
        self.optimizer = optimizer

    def on_epoch_start(self):
        self.model.train()

    def batch_update(self, x, y):
        self.optimizer.zero_grad()
        loss = self.model(x, y)
        loss = sum(l for l in loss.values())
        loss.backward()
        self.optimizer.step()
        return loss

num_classes = NUM_CLASSES + 1
device = torch.device(DEVICE)

model_ft = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
in_features = model_ft.roi_heads.box_predictor.cls_score.in_features
model_ft.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
in_features_mask = model_ft.roi_heads.mask_predictor.conv5_mask.in_channels
hidden_layer = 256
model_ft.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

# Усі шари навчаються
for param in model_ft.parameters():
    param.requires_grad = True

# Заморожуємо всі параметри, крім нових
#for param in model_ft.parameters():
    #param.requires_grad = False

# Розморожуємо тільки нові шари
#for param in model_ft.roi_heads.box_predictor.parameters():
    #param.requires_grad = True

#for param in model_ft.roi_heads.mask_predictor.parameters():
    #param.requires_grad = True

#optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.001)

optimizer = torch.optim.SGD( model_ft.parameters(),lr=0.001, momentum= 0.9, weight_decay=0.0005)

train_epoch = TrainEpoch(
    model_ft,
    loss=None,
    metrics=None,
    optimizer=optimizer,
    device=DEVICE,
    verbose=True,
)



class ValidEpoch(MyEpoch):
    def __init__(self, model, loss=None, metrics=None, device='cuda', verbose=True):
        """
        A class for processing validation data.

        Parameters:
        - model: torch.nn.Module — model for validation.
        - loss: loss function (or None if losses are built into the model).
        - metrics: list of metrics (or None).
        - device: str — the device on which the validation is performed ('cpu' or 'cuda').
        - verbose: bool — whether to output the progress log.
        """
        super().__init__(
            model=model,
            loss=loss,
            metrics=metrics,
            stage_name='valid',
            device=device,
            verbose=verbose,
        )

    def on_epoch_start(self):
        """Switches the model to evaluation mode."""
        self.model.eval()

    @torch.no_grad()
    def batch_update(self, x, y):
        """
        Calculates losses for one batch without updating the weights.

        Parameters:
        - x: data (image).
        - y: labels (annotations).

        Returns:
        - loss: losses for this batch.
        """
        outputs = self.model(x, y)
        loss = sum(l for l in outputs.values())  # Grieves losses, if there are several of them
        return loss


# Function IoU
def compute_iou(y_true, y_pred):
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()
    return jaccard_score(y_true, y_pred, average='macro')

# ValidEpoch initialization with IoU
valid_epoch = ValidEpoch(
    model=model_ft,
    loss=None,
    metrics={'IoU': compute_iou},  # We connect the metric
    device=DEVICE,
    verbose=True
)

# Training cycle
best_iou = 0.0  # Initialize a variable to save the best model
for epoch in range(NUM_EPOCHS):
    print(f"\nEpoch {epoch+1}/{NUM_EPOCHS}")

    # Training
    train_logs = train_epoch.run(data_loader_train)
    print(f"Train logs: {train_logs}")

    # Validation
    val_logs = valid_epoch.run(data_loader_val)
    print(f"Validation logs: {val_logs}")

    # Saving the best model
    if val_logs['IoU'] > best_iou:
        best_iou = val_logs['IoU']
        torch.save(model_ft.state_dict(), f'best_model_epoch{epoch+1}_iou{val_logs["IoU"]:.4f}.pth')
        print("Model saved!")